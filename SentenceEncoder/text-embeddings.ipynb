{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import time\n",
    "\n",
    "from elasticsearch import Elasticsearch\n",
    "from elasticsearch.helpers import bulk\n",
    "\n",
    "# Use tensorflow 1 behavior to match the Universal Sentence Encoder\n",
    "# examples (https://tfhub.dev/google/universal-sentence-encoder/2).\n",
    "import tensorflow.compat.v1 as tf\n",
    "import tensorflow_hub as hub\n",
    "\n",
    "##### INDEXING #####\n",
    "\n",
    "def index_data():\n",
    "    print(\"Creating the 'posts' index.\")\n",
    "    client.indices.delete(index=INDEX_NAME, ignore=[404])\n",
    "\n",
    "    with open(INDEX_FILE) as index_file:\n",
    "        source = index_file.read().strip()\n",
    "        client.indices.create(index=INDEX_NAME, body=source)\n",
    "\n",
    "    docs = []\n",
    "    count = 0\n",
    "\n",
    "    with open(DATA_FILE) as data_file:\n",
    "        for line in data_file:\n",
    "            line = line.strip()\n",
    "\n",
    "            doc = json.loads(line)\n",
    "            if doc[\"type\"] != \"question\":\n",
    "                continue\n",
    "\n",
    "            docs.append(doc)\n",
    "            count += 1\n",
    "\n",
    "            if count % BATCH_SIZE == 0:\n",
    "                index_batch(docs)\n",
    "                docs = []\n",
    "                print(\"Indexed {} documents.\".format(count))\n",
    "\n",
    "        if docs:\n",
    "            index_batch(docs)\n",
    "            print(\"Indexed {} documents.\".format(count))\n",
    "\n",
    "    client.indices.refresh(index=INDEX_NAME)\n",
    "    print(\"Done indexing.\")\n",
    "\n",
    "def index_batch(docs):\n",
    "    titles = [doc[\"title\"] for doc in docs]\n",
    "    title_vectors = embed_text(titles)\n",
    "\n",
    "    requests = []\n",
    "    for i, doc in enumerate(docs):\n",
    "        request = doc\n",
    "        request[\"_op_type\"] = \"index\"\n",
    "        request[\"_index\"] = INDEX_NAME\n",
    "        request[\"title_vector\"] = title_vectors[i]\n",
    "        requests.append(request)\n",
    "    bulk(client, requests)\n",
    "\n",
    "##### SEARCHING #####\n",
    "\n",
    "def run_query_loop():\n",
    "    while True:\n",
    "        try:\n",
    "            handle_query()\n",
    "        except KeyboardInterrupt:\n",
    "            return\n",
    "\n",
    "def handle_query():\n",
    "    query = input(\"Enter query: \")\n",
    "\n",
    "    embedding_start = time.time()\n",
    "    query_vector = embed_text([query])[0]\n",
    "    embedding_time = time.time() - embedding_start\n",
    "\n",
    "    script_query = {\n",
    "        \"script_score\": {\n",
    "            \"query\": {\"match_all\": {}},\n",
    "            \"script\": {\n",
    "                \"source\": \"cosineSimilarity(params.query_vector, doc['title_vector']) + 1.0\",\n",
    "                \"params\": {\"query_vector\": query_vector}\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "\n",
    "    search_start = time.time()\n",
    "    response = client.search(\n",
    "        index=INDEX_NAME,\n",
    "        body={\n",
    "            \"size\": SEARCH_SIZE,\n",
    "            \"query\": script_query,\n",
    "            \"_source\": {\"includes\": [\"title\", \"body\"]}\n",
    "        }\n",
    "    )\n",
    "    search_time = time.time() - search_start\n",
    "\n",
    "    print()\n",
    "    print(\"{} total hits.\".format(response[\"hits\"][\"total\"][\"value\"]))\n",
    "    print(\"embedding time: {:.2f} ms\".format(embedding_time * 1000))\n",
    "    print(\"search time: {:.2f} ms\".format(search_time * 1000))\n",
    "    for hit in response[\"hits\"][\"hits\"]:\n",
    "        print(\"id: {}, score: {}\".format(hit[\"_id\"], hit[\"_score\"]))\n",
    "        print(hit[\"_source\"])\n",
    "        print()\n",
    "\n",
    "##### EMBEDDING #####\n",
    "\n",
    "def embed_text(text):\n",
    "    vectors = session.run(embeddings, feed_dict={text_ph: text})\n",
    "    return [vector.tolist() for vector in vectors]\n",
    "\n",
    "##### MAIN SCRIPT #####\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    INDEX_NAME = \"posts\"\n",
    "    INDEX_FILE = \"data/posts/index.json\"\n",
    "\n",
    "    DATA_FILE = \"data/posts/posts.json\"\n",
    "    BATCH_SIZE = 1000\n",
    "\n",
    "    SEARCH_SIZE = 5\n",
    "\n",
    "    print(\"Downloading pre-trained embeddings from tensorflow hub...\")\n",
    "    embed = hub.Module(\"https://tfhub.dev/google/universal-sentence-encoder/2\")\n",
    "    text_ph = tf.placeholder(tf.string)\n",
    "    embeddings = embed(text_ph)\n",
    "    print(\"Done.\")\n",
    "\n",
    "    print(\"Creating tensorflow session...\")\n",
    "    session = tf.Session()\n",
    "    session.run(tf.global_variables_initializer())\n",
    "    session.run(tf.tables_initializer())\n",
    "    print(\"Done.\")\n",
    "\n",
    "    client = Elasticsearch()\n",
    "\n",
    "    index_data()\n",
    "    run_query_loop()\n",
    "\n",
    "    print(\"Closing tensorflow session...\")\n",
    "    session.close()\n",
    "    print(\"Done.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit",
   "name": "python385jvsc74a57bd0fcbb1ef9f507839a1a8deab4eeab6ce456c0aa3e9723d1242394540b18e281aa"
  },
  "language_info": {
   "name": "python",
   "nbconvert_exporter": "python",
   "version": "3.8.5"
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}